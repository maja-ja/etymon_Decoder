import json
import os
import shutil
from datetime import datetime

DB_FILE = 'etymon_database.json'
PENDING_FILE = 'pending_data.json'
BACKUP_DIR = 'backups' # å¢åŠ å‚™ä»½è³‡æ–™å¤¾

def load_json(filename):
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                return data
            except json.JSONDecodeError:
                print(f"âŒ éŒ¯èª¤ï¼š{filename} æ ¼å¼æå£ã€‚")
                return None
    return None

def merge_data():
    # 1. è®€å–å¾…åˆä½µè³‡æ–™
    pending = load_json(PENDING_FILE)
    if not pending:
        print(f"â„¹ï¸ æç¤ºï¼š{PENDING_FILE} æ˜¯ç©ºçš„æˆ–ä¸å­˜åœ¨ï¼Œç„¡éœ€åˆä½µã€‚")
        return

    # 2. è®€å–ä¸»è³‡æ–™åº«ï¼ˆè‹¥ä¸å­˜åœ¨å‰‡å»ºç«‹ç©ºä¸²åˆ—ï¼‰
    main_db = load_json(DB_FILE)
    if main_db is None and os.path.exists(DB_FILE):
        print("âš ï¸ è­¦å‘Šï¼šä¸»è³‡æ–™åº«æå£ï¼Œåœæ­¢åˆä½µä»¥é˜²è¦†è“‹ã€‚")
        return
    main_db = main_db or []

    # 3. å‚™ä»½ä¸»è³‡æ–™åº« (å®‰å…¨ç¬¬ä¸€ï¼)
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if os.path.exists(DB_FILE):
        shutil.copy(DB_FILE, f"{BACKUP_DIR}/db_backup_{timestamp}.json")

    # ç¢ºä¿ pending æ ¼å¼çµ±ä¸€
    if isinstance(pending, dict):
        pending = [pending]

    # 4. é–‹å§‹åˆä½µé‚è¼¯ (ä¿ç•™ä½ åŸæœ¬å„ªè‰¯çš„å»é‡é‚è¼¯)
    for new_cat in pending:
        cat_name = new_cat.get("category")
        target_cat = next((c for c in main_db if c["category"] == cat_name), None)
        
        if not target_cat:
            main_db.append(new_cat)
            print(f"â• å·²æ–°å¢å…¨æ–°åˆ†é¡ï¼š{cat_name}")
        else:
            for new_group in new_cat.get("root_groups", []):
                new_roots = set(new_group["roots"])
                target_group = next((g for g in target_cat["root_groups"] 
                                   if set(g["roots"]) == new_roots), None)
                
                if not target_group:
                    target_cat["root_groups"].append(new_group)
                    print(f"  â””â”€ ğŸš€ æ–°å¢å­—æ ¹çµ„ï¼š{', '.join(new_group['roots'])}")
                else:
                    existing_words = {v["word"] for v in target_group["vocabulary"]}
                    added_count = 0
                    for v in new_group["vocabulary"]:
                        if v["word"] not in existing_words:
                            target_group["vocabulary"].append(v)
                            existing_words.add(v["word"])
                            added_count += 1
                    print(f"  â””â”€ ğŸ”„ åˆä½µå­—æ ¹çµ„ {', '.join(new_group['roots'])}ï¼Œæ–°å¢ {added_count} å–®å­—")

    # 5. å¯«å›ä¸»è³‡æ–™åº«
    with open(DB_FILE, 'w', encoding='utf-8') as f:
        json.dump(main_db, f, ensure_ascii=False, indent=2)
    
    # 6. ã€é‡è¦ã€‘åˆä½µæˆåŠŸå¾Œï¼Œæ¸…ç©º Pending æª”æ¡ˆé˜²æ­¢é‡è¤‡åˆä½µ
    with open(PENDING_FILE, 'w', encoding='utf-8') as f:
        json.dump([], f) # å¯«å›ç©ºé™£åˆ—
    
    print(f"\nâœ… åˆä½µå®Œæˆï¼è³‡æ–™å·²å„²å­˜è‡³ {DB_FILE}ï¼Œ{PENDING_FILE} å·²æ¸…ç©ºã€‚")

if __name__ == "__main__":
    merge_data()
